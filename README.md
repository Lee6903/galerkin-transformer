# Fourier Transformer and Galerkin Transformer: Attention without softmax
This is the repository for paper:

# Examples

## Burgers

## Interface Darcy flow

## Inverse coefficient identification for Darcy flow

# Acknowledgement
The hardware to perform this work is provided by Andromeda Saving Fund. This work was supported in part by the National Science Foundation under grants DMS-1913080. We would like to thank [Dr. Long Chen (UC Irvine)](github.com/lyc102) for the inspiration and encouragement on the initial conceiving of this paper. We would like to thank Dr. Ruchi Guo (UC Irvine) and Dr. Yuanzhe Xi (Emory) for some early feedback on the choice of the numerical experiements. We would like to thank [Mr. Zongyi Li (Caltech)](https://github.com/zongyi-li) for sharing some early dev code in the updated PyTorch fft interface.  We would like to thank Joel Schlosser(Facebook) to incorporate our change to the PyTorch transformer module to simplify our testing pipeline.  We would be grateful to the PyTorch community for selflessly code sharing, including Phil Wang([lucidrains@github](https://github.com/lucidrains)) and [Harvard NLP group Klein et al. (2017)](https://nlp.seas.harvard.edu/2018/04/03/attention.html).  Wewould like to thank the chebfun Driscoll et al. (2014) for integrating powerful tools into a simple interface to solve PDEs. We would like to thank Yannic Kilcher (ykilcher@twitter)for frequently covering the newest research on Transformers in video formats.  We wouldalso like to thank the Python community (Van Rossum and Drake (2009); Oliphant (2007))for sharing and developing the tools that enabled this work, including Pytorch Paszke et al.(2017),  NumPy Harris et al. (2020),  SciPy Virtanen et al. (2020),  Plotly Inc. (2015) andMatplotlib Hunter (2007).
